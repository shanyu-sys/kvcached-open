# Multi-LLM Benchmark Configuration

# Benchmark settings
benchmark:
  start_timestamp: "2025-08-07 00:00:00.000000" # start timestamp of the trace, this will be ignored
  end_timestamp: "2025-08-08 01:00:00.000000" # end timestamp of the trace
  slowdown_factor: 1 # a value of 0.1 means the replay is 10 times faster.
  torch_master_addr: "localhost" # master address for torch distributed communication
  torch_master_port: "29500" # default port for torch distributed communication
  status_interval: 60 # benchmark status update interval in seconds

# Instance configurations
instances:
  - name: "Qwen3-0.6B"
    model_name: "Qwen/Qwen3-0.6B"
    port: 30101
    trace_file: null
    trace_model_filter: "Qwen3-0.6B_2025-08-13"
    model_path: null  # Set to null to use model_name, or specify a local path to use offline models.
    gpu_id: 0

  - name: "Llama-3.2-1B"
    model_name: "meta-llama/Llama-3.2-1B"
    port: 30102
    trace_file: null
    trace_model_filter: "Llama-3.2-1B_preview"
    model_path: null
    gpu_id: 1

  - name: "gemma-2-2b-it"
    model_name: "google/gemma-2-2b-it"
    port: 30103
    trace_file: null
    trace_model_filter: "gemma-2-2b-it_2025-07-09"
    model_path: null
    gpu_id: 2

  - name: "Llama-3.2-3B"
    model_name: "meta-llama/Llama-3.2-3B"
    port: 30104
    trace_file: null
    trace_model_filter: "Llama-3.2-3B_latest"
    model_path: null
    gpu_id: 3

  - name: "Qwen3-4B"
    model_name: "Qwen/Qwen3-4B"
    port: 30105
    trace_file: null
    trace_model_filter: "Qwen3-4B_0814"
    model_path: null  # Set to null to use model_name, or specify a local path to use offline models.
    gpu_id: 4

  - name: "Mistral-7B-Instruct-v0.2"
    model_name: "mistralai/Mistral-7B-Instruct-v0.2"
    port: 30106
    trace_file: null
    trace_model_filter: "Mistral-7B-Instruct-v0.2_0814"
    model_path: null
    gpu_id: 5

  - name: "Llama-3.1-8B-Instruct"
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    port: 30107
    trace_file: null
    trace_model_filter: "Llama-3.1-8B-Instruct_1024"
    model_path: null
    gpu_id: 6

  - name: "Mistral-Nemo-Instruct-2407"
    model_name: "mistralai/Mistral-Nemo-Instruct-2407"
    port: 30108
    trace_file: null
    trace_model_filter: "Mistral-Nemo-Instruct-2407_20251022"
    model_path: null
    gpu_id: 7


