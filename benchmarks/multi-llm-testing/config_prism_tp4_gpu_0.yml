# Multi-LLM Benchmark Configuration

# Benchmark settings
benchmark:
  start_timestamp: "2025-08-07 00:00:00.000000" # start timestamp of the trace, this will be ignored
  end_timestamp: "2025-08-08 00:00:20.000000" # end timestamp of the trace
  duration: "10m" # duration from start_timestamp (e.g., "30s", "10m", "1h", "1.5h"). If specified, overrides end_timestamp.
  slowdown_factor: 1 # a value of 0.1 means the replay is 10 times faster.
  torch_master_addr: "localhost" # master address for torch distributed communication
  torch_master_port: "29500" # default port for torch distributed communication
  status_interval: 60 # benchmark status update interval in seconds
  gpu_id: 0
  tp: 4 

# Instance configurations
instances:
  - name: "Meta-Llama-3-70B-1"
    model_name: "meta-llama/Meta-Llama-3-70B"
    port: 30001
    trace_file: null
    trace_model_filter: "Meta-Llama-3-70B_preview"
    model_path: null  # Set to null to use model_name, or specify a local path to use offline models.
    tp: 4

  - name: "Meta-Llama-3-70B-2"
    model_name: "Meta-Llama-3-70B"
    port: 30002
    trace_file: null
    trace_model_filter: "Llama-3.1-70B-Instruct_latest"
    model_path: null
    tp: 4

  - name: "Qwen3-32B-1"
    model_name: "Qwen/Qwen3-32B"
    port: 30003
    trace_file: null
    trace_model_filter: "Qwen3-32B_preview"
    model_path: null
    tp: 4

  - name: "Qwen3-32B-2"
    model_name: "Qwen/Qwen3-32B"
    port: 30004
    trace_file: null
    trace_model_filter: "Qwen3-32B_20251022"
    model_path: null
    tp: 4
