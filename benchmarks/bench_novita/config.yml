# Multi-LLM Benchmark Configuration

# Benchmark settings
benchmark:
  start_timestamp: "2025-04-06 00:00:00.000000" # start timestamp of the trace
  end_timestamp: "2025-04-06 00:10:00.000000" # end timestamp of the trace
  slowdown_factor: 0.1 # a value of 0.1 means the replay is 10 times faster.
  torch_master_addr: "localhost" # master address for torch distributed communication
  torch_master_port: "29500" # default port for torch distributed communication
  status_interval: 60 # benchmark status update interval in seconds

# Instance configurations
instances:
  - name: "NousResearch_Hermes_2_Pro_Llama_3_8B"
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    port: 30001
    trace_file: "/data/novita_trace/NousResearch_Hermes_2_Pro_Llama_3_8B_2025_04_07_30d.csv"
    model_path: null  # Set to null to use model_name, or specify a local path to use offline models.

  - name: "deepseek_ai_DeepSeek_R1_Distill_Llama_8B"
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    port: 30002
    trace_file: "/data/novita_trace/deepseek_ai_DeepSeek_R1_Distill_Llama_8B_2025_04_07_30d.csv"
    model_path: null

  - name: "meta_llama_Llama_3.2_1B_Instruct_1"
    model_name: "meta-llama/Llama-3.2-1B-Instruct"
    port: 30003
    trace_file: "/data/novita_trace/meta_llama_Llama_3.2_1B_Instruct_2025_04_07_30d.csv"
    model_path: null

  - name: "meta_llama_Llama_3.2_1B_Instruct_2"
    model_name: "meta-llama/Llama-3.2-1B-Instruct"
    port: 30004
    trace_file: "/data/novita_trace/meta_llama_Llama_3.2_1B_Instruct_2025_04_07_30d.csv"
    model_path: null

